{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T21:37:34.726781Z",
     "start_time": "2024-06-02T21:37:17.493229Z"
    }
   },
   "source": [
    "#Nikolaos Giannopoulos AM 5199\n",
    "#Georgios Strouggis AM 5357\n",
    "\n",
    "#All the imports happen here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#1 Dimensional Reduction\n",
    "\n",
    "d = 784\n",
    "\n",
    "#Load the data\n",
    "train_data = pd.read_csv('fashion-mnist_train.csv')\n",
    "test_data = pd.read_csv('fashion-mnist_test.csv')\n",
    "\n",
    "#Make sure it got loaded correctly\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "#Print a few data form each data set\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nFirst few rows of testing data:\")\n",
    "print(test_data.head())\n",
    "\n",
    "#Initialize number of samples for each category\n",
    "n = 1000\n",
    "\n",
    "#Sample n images for each category from the training data\n",
    "sampled_data_list = []\n",
    "for label in train_data['label'].unique():\n",
    "    sampled_data = train_data[train_data['label'] == label].sample(n=n, random_state=42)\n",
    "    sampled_data_list.append(sampled_data)\n",
    "train_data_sample = pd.concat(sampled_data_list).reset_index(drop=True)\n",
    "\n",
    "#Split images and labels as X and Y\n",
    "X_train = train_data_sample.drop(columns=['label'], axis=1).values\n",
    "X_test = test_data.drop(columns=['label'], axis=1).values\n",
    "Y_train = train_data_sample['label'].values\n",
    "Y_test = test_data['label'].values\n",
    "\n",
    "#Use Standard Scaler for the images\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Finalize Y\n",
    "y_train = Y_train\n",
    "y_test = Y_test\n",
    "\n",
    "#Resize to 28x28\n",
    "x_train = X_train_scaled.reshape((-1, 28, 28))\n",
    "x_test = X_test_scaled.reshape((-1, 28, 28))\n",
    "\n",
    "#View Train Data as pictures\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='viridis')\n",
    "plt.show()"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:37:44.457925Z",
     "start_time": "2024-06-02T21:37:39.270081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reduce dimensionality through PCA while keeping 90% covariance\n",
    "pca = PCA(n_components=0.90)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Print the dimensionality\n",
    "M = X_train_pca.shape[1]\n",
    "print(f\"Dimensionality: {M}\")\n",
    "\n",
    "#Resize to 12x11. These dimensions were chosen because the square root of 132 is a floating point number, so instead we need to use values for its dimensions that are not equals, such as 12x11=132.\n",
    "x_train_pca = X_train_pca.reshape((-1, 12, 11))\n",
    "x_test_pca = X_test_pca.reshape((-1, 12, 11))\n",
    "\n",
    "# View Train Data after PCA as pictures\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(x_train_pca[i], cmap='viridis')\n",
    "plt.show()"
   ],
   "id": "6876100728fdf7a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:38:36.474210Z",
     "start_time": "2024-06-02T21:37:57.150963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential( #Encoding\n",
    "            nn.Linear(d, d//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d//4, M),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential( #Decoding\n",
    "            nn.Linear(M, d//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d//4, d),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "#Normalize data\n",
    "X_train_new = X_train.astype(np.float32) / 255.0\n",
    "X_test_new = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "#Transform to tensors\n",
    "x_train_torch = t.tensor(X_train_new, dtype=t.float32)\n",
    "x_test_torch = t.tensor(X_test_new, dtype=t.float32)\n",
    "\n",
    "#Create DataLoader. Last time we did it hard coded and had many challenges. Datasets and Loader seem to be more efficient\n",
    "#Batch size 50 like last time\n",
    "train_dataset = TensorDataset(x_train_torch, x_test_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "#Model, criterion and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Set number of epochs, the more epochs, the better the outcome\n",
    "#epochs = 5\n",
    "#epochs = 10\n",
    "#epochs = 20\n",
    "epochs = 20\n",
    "\n",
    "#Start of training loop\n",
    "for epoch in range(epochs):\n",
    "    for data in train_loader:\n",
    "        inputs, _ = data\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "#Encode and decode images\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    encoded_data = model.encoder(x_train_torch)\n",
    "    decoded_data = model.decoder(encoded_data).numpy()\n",
    "\n",
    "#Visualize original and reconstructed images\n",
    "r = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(r):\n",
    "    #Display original\n",
    "    plot = plt.subplot(2, r, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='viridis')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    #Display reconstructed\n",
    "    plot = plt.subplot(2, r, i + 1 + r)\n",
    "    plt.imshow(decoded_data[i].reshape(28, 28), cmap='viridis')\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "5707eb873a41fab4",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:39:18.003522Z",
     "start_time": "2024-06-02T21:38:56.966184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Second best from last exercise because best didnt work well here\n",
    "#First we need the data to be encoded so we use them\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.eval()\n",
    "with t.no_grad():\n",
    "    train_tensor_encoded = autoencoder.encoder(t.tensor(X_train_scaled, dtype=t.float32))\n",
    "    test_tensor_encoded = autoencoder.encoder(t.tensor(X_test_scaled, dtype=t.float32))\n",
    "\n",
    "#We save them on the array, although they are not array just to differentiate them\n",
    "x_train_array = train_tensor_encoded.numpy()\n",
    "x_test_array = test_tensor_encoded.numpy()\n",
    "\n",
    "#Create and train the Random Forest with the 100 estimators\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(x_train_array, y_train)\n",
    "#Forest accuracy\n",
    "accuracy = random_forest.score(x_test_array, y_test)\n",
    "print(f\"Accuracy with Random Forest on Autoencoder: {accuracy}\\n\")\n",
    "\n",
    "#Create and train the Random Forest with the 100 estimators\n",
    "random_forest2 = RandomForestClassifier(n_estimators=100)\n",
    "random_forest2.fit(X_train_pca, y_train)\n",
    "#Forest accuracy\n",
    "accuracy = random_forest2.score(X_test_pca, y_test)\n",
    "print(f\"Accuracy with Random Forest on PCA: {accuracy}\\n\")"
   ],
   "id": "604a59c655029088",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:39:49.756455Z",
     "start_time": "2024-06-02T21:39:22.279935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2 Clustering\n",
    "#k-means for PCA\n",
    "silhouette_scores = []\n",
    "k_list = []\n",
    "\n",
    "#Loop for k values with random_state equals 42 after research on which should be\n",
    "for k in range(10, 21):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_train_pca)\n",
    "    score = silhouette_score(X_train_pca, cluster_labels, metric='euclidean')\n",
    "    silhouette_scores.append(score)\n",
    "    k_list.append(kmeans)\n",
    "    print(f'Cluster: {k}, Score: {score}')\n",
    "\n",
    "#Best value for k and score\n",
    "k_best = k_list[np.argmax(silhouette_scores)]\n",
    "k_best_score = 0\n",
    "k_best_pos = 0\n",
    "kmeans_best = 0\n",
    "for i in range(10):\n",
    "    if silhouette_scores[i] > k_best_score:\n",
    "        k_best_score = silhouette_scores[i]\n",
    "        k_best_pos = i + 10\n",
    "        kmeans_best = k_list[i]\n",
    "print(f\"Optimal number of clusters: {k_best_pos} with a score of {k_best_score}\")\n",
    "center = k_best.cluster_centers_\n",
    "\n",
    "#Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(10, 21), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for different K values')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Decode the center and reconstruct the images to their original dimensions\n",
    "center_tensor = t.tensor(center, dtype=t.float32)\n",
    "with t.no_grad():\n",
    "    reconstructed_center = model.decoder(center_tensor).numpy()\n",
    "\n",
    "#Plot the centers\n",
    "plt.figure(figsize=(20, 4))\n",
    "for j in range(k_best_pos):\n",
    "    plt.subplot(2, k_best_pos // 2, j + 1)\n",
    "    plt.imshow(reconstructed_center[j].reshape(28, 28), cmap='viridis')\n",
    "    plt.title(f'Cluster {j+1}')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "17b9edc7aa2cab14",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:39:52.477302Z",
     "start_time": "2024-06-02T21:39:52.429591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "#Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y_test_encoded = label_encoder.fit_transform(Y_test)\n",
    "\n",
    "cluster_labels = kmeans_best.predict(X_train_pca)\n",
    "num_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "#Empty majority class array\n",
    "majority_classes = np.zeros(num_clusters, dtype=int)\n",
    "\n",
    "#Purity\n",
    "purity = 0\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    cluster_labels_true = Y_test_encoded[cluster_indices]\n",
    "    if len(cluster_labels_true) > 0:\n",
    "        majority_class = np.argmax(np.bincount(cluster_labels_true))\n",
    "        purity += np.sum(cluster_labels_true == majority_class)\n",
    "\n",
    "purity /= len(Y_test)\n",
    "print(f'Purity: {purity}')\n",
    "\n",
    "\n",
    "#We set the loop\n",
    "f_measures = []\n",
    "total_samples = len(y_train)\n",
    "#Loop start for each cluster\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    cluster_labels_true = y_train[cluster_indices]\n",
    "    #Calculate TP, FP and FN\n",
    "    if len(cluster_labels_true) > 0:\n",
    "        majority_class = np.argmax(np.bincount(cluster_labels_true))\n",
    "        TP = np.sum(cluster_labels_true == majority_class)\n",
    "        FP = len(cluster_labels_true) - TP\n",
    "        FN = np.sum(y_train == majority_class) - TP\n",
    "        \n",
    "        #Calculate precision recall and f-measure and save final results\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f_measure = (1 + TP) * (precision * recall) / (TP * precision + recall) if (TP * precision + recall) > 0 else 0\n",
    "        weighted_f_measure = f_measure * (len(cluster_labels_true) / total_samples)\n",
    "        f_measures.append(weighted_f_measure)\n",
    "\n",
    "#Calculate total and print final results\n",
    "f_measure_total = np.sum(f_measures)\n",
    "print(f'F-measure: {f_measure_total}')"
   ],
   "id": "72b97ed91a7732d",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:40:16.491335Z",
     "start_time": "2024-06-02T21:39:55.664940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#k-means for Autoencoder\n",
    "silhouette_scores = []\n",
    "k_list = []\n",
    "\n",
    "\n",
    "for k in range(10, 21):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(encoded_data)\n",
    "    score = silhouette_score(encoded_data, cluster_labels, metric='euclidean')\n",
    "    silhouette_scores.append(score)\n",
    "    k_list.append(kmeans)\n",
    "    print(f'Cluster: {k}, Score: {score}')\n",
    "\n",
    "#Best value for k and score\n",
    "k_best = k_list[np.argmax(silhouette_scores)]\n",
    "k_best_score = 0\n",
    "k_best_pos = 0\n",
    "kmeans_best = 0\n",
    "for i in range(10):\n",
    "    if silhouette_scores[i] > k_best_score:\n",
    "        k_best_score = silhouette_scores[i]\n",
    "        k_best_pos = i + 10\n",
    "        kmeans_best = k_list[i]\n",
    "print(f\"Optimal number of clusters: {k_best_pos} with a score of {k_best_score}\")\n",
    "center = k_best.cluster_centers_\n",
    "\n",
    "#Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(10, 21), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for different K values')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Decode the center and reconstruct the images to their original dimensions\n",
    "center_tensor = t.tensor(center, dtype=t.float32)\n",
    "with t.no_grad():\n",
    "    reconstructed_center = model.decoder(center_tensor).numpy()\n",
    "\n",
    "#Plot the centers\n",
    "plt.figure(figsize=(20, 4))\n",
    "for j in range(k_best_pos):\n",
    "    plt.subplot(2, k_best_pos // 2, j + 1)\n",
    "    plt.imshow(reconstructed_center[j].reshape(28, 28), cmap='viridis')\n",
    "    plt.title(f'Cluster {j+1}')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "id": "db53e5c4684fb458",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T21:40:18.270999Z",
     "start_time": "2024-06-02T21:40:18.245208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y_test_encoded = label_encoder.fit_transform(Y_test)\n",
    "\n",
    "cluster_labels = kmeans_best.predict(encoded_data)\n",
    "num_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "#Empty majority class array\n",
    "majority_classes = np.zeros(num_clusters, dtype=int)\n",
    "\n",
    "#Purity\n",
    "purity = 0\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    cluster_labels_true = Y_test_encoded[cluster_indices]\n",
    "    if len(cluster_labels_true) > 0:\n",
    "        majority_class = np.argmax(np.bincount(cluster_labels_true))\n",
    "        purity += np.sum(cluster_labels_true == majority_class)\n",
    "\n",
    "purity /= len(Y_test)\n",
    "print(f'Purity: {purity}')\n",
    "\n",
    "\n",
    "\n",
    "#We set the loop\n",
    "f_measures = []\n",
    "total_samples = len(y_train)\n",
    "#Loop start for each cluster\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    cluster_labels_true = y_train[cluster_indices]\n",
    "    #Calculate TP, FP and FN\n",
    "    if len(cluster_labels_true) > 0:\n",
    "        majority_class = np.argmax(np.bincount(cluster_labels_true))\n",
    "        TP = np.sum(cluster_labels_true == majority_class)\n",
    "        FP = len(cluster_labels_true) - TP\n",
    "        FN = np.sum(y_train == majority_class) - TP\n",
    "        \n",
    "        #Calculate precision recall and f-measure and save final results\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f_measure = (1 + TP) * (precision * recall) / (TP * precision + recall) if (TP * precision + recall) > 0 else 0\n",
    "        weighted_f_measure = f_measure * (len(cluster_labels_true) / total_samples)\n",
    "        f_measures.append(weighted_f_measure)\n",
    "\n",
    "#Calculate total and print final results\n",
    "f_measure_total = np.sum(f_measures)\n",
    "print(f'F-measure: {f_measure_total}')"
   ],
   "id": "7c2a6c9129c7770c",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "e336b8c688ba27af",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
