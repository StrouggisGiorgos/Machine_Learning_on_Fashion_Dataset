{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T15:50:55.933591Z",
     "start_time": "2024-05-26T15:50:41.264783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Nikolaos Giannopoulos AM 5199\n",
    "#Georgios Strouggis AM 5357\n",
    "\n",
    "#All the imports happen here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#Load the data\n",
    "train_data = pd.read_csv('fashion-mnist_train.csv')\n",
    "test_data = pd.read_csv('fashion-mnist_test.csv')\n",
    "\n",
    "#Make sure it got loaded correctly\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "#Print a few data form each data set\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nFirst few rows of testing data:\")\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "#Split images and labels as X and Y\n",
    "X_train = train_data.drop(columns=['label'], axis=1).values\n",
    "X_test = test_data.drop(columns=['label'], axis=1).values\n",
    "Y_train = train_data['label'].values\n",
    "Y_test = test_data['label'].values\n",
    "\n",
    "#Use Standard scaler for the images\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Finallize Y\n",
    "y_train = Y_train\n",
    "y_test = Y_test\n",
    "\n",
    "#Pictures of 28x28\n",
    "x_train = X_train_scaled.reshape((-1, 28, 28))\n",
    "x_test = X_test_scaled.reshape((-1, 28, 28))\n",
    "\n",
    "#View Train Data as pictures\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(x_train[i], cmap='viridis')\n",
    "plt.show()"
   ],
   "id": "894f1fe53214d990",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Convert the data from numpy to tensors\n",
    "x_train_tensor = t.tensor(x_train, dtype=t.float32)\n",
    "x_test_tensor = t.tensor(x_test, dtype=t.float32)\n",
    "\n",
    "#Reshape the tensors to match the expected input shape \n",
    "x_train_tensor = x_train_tensor.unsqueeze(1)\n",
    "x_test_tensor = x_test_tensor.unsqueeze(1)\n",
    "\n",
    "#Apply max polling to train/test data\n",
    "x_train_polled = t.max_pool2d(x_train_tensor, kernel_size=4)\n",
    "x_test_polled = t.max_pool2d(x_test_tensor, kernel_size=4)\n",
    "\n",
    "#Convert the pooled tensors back to numpy arrays\n",
    "x_train_np = x_train_polled.squeeze(1).numpy()  \n",
    "x_test_np = x_test_polled.squeeze(1).numpy()\n",
    "\n",
    "#Print to test if all went accordingly\n",
    "print(f\"Train data shape: {x_train_np.shape}\")\n",
    "print(f\"Test data shape: {x_test_np.shape}\")\n",
    "\n",
    "#Array of polled train/test data\n",
    "x_train_array = x_train_np.reshape(x_train_np.shape[0], -1)\n",
    "x_test_array = x_test_np.reshape(x_test_np.shape[0], -1)\n",
    "\n",
    "#Print to test that the conversation to array worked\n",
    "print(f\"Train data array shape: {x_train_array.shape}\")\n",
    "print(f\"Test data array shape: {x_test_array.shape}\")\n",
    "\n",
    "\n",
    "#Print of the polled images to certify the max polling\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(x_train_np[i], cmap='viridis')\n",
    "plt.show()\n"
   ],
   "id": "dc51fbeeb6ac77e2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Define different values for K\n",
    "k_values = [1, 3, 5]\n",
    "\n",
    "#Loop for different K values, then create classifier, train it and finally test it\n",
    "for k in k_values:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn_classifier.fit(x_train_array, y_train)\n",
    "    accuracy = knn_classifier.score(x_test_array, y_test)\n",
    "    \n",
    "#Prints of report for each value\n",
    "    print(f\"Accuracy with K={k}: {accuracy}\\n\")\n",
    "    print(f\"Classification Report: {classification_report(y_test, y_pred=knn_classifier.predict(x_test_array))}\")\n",
    "    print(\"----------------------------\")"
   ],
   "id": "a40e176fd5feec02",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Create and train the Decision Tree with max depth of 10\n",
    "decision_tree = DecisionTreeClassifier(max_depth=10)\n",
    "decision_tree.fit(x_train_array, y_train)\n",
    "\n",
    "#Create and train the Random Forest with the 100 estimators\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(x_train_array, y_train)\n",
    "#Forest accuracy\n",
    "accuracy = random_forest.score(x_test_array, y_test)\n",
    "print(f\"Accuracy with Random Forest: {accuracy}\\n\")\n",
    "\n",
    "#Plot creator for the Decision Tree\n",
    "print(\"The Decision Tree:\\n\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(decision_tree, filled=True, feature_names=[str(i) for i in range(49)], class_names=[str(i) for i in range(10)])\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "d6a56008377b4c1a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "#Definiton of C and parameter values\n",
    "C_values = [1, 10, 100]\n",
    "param_values = [0.02, 0.1, 1]\n",
    "\n",
    "#Due to the complexity we are forced to use a subset of data\n",
    "subset = []\n",
    "for label in np.unique(y_train):\n",
    "    label_values = np.where(y_train == label)[0]\n",
    "    np.random.shuffle(label_values)\n",
    "    num_train = int(len(label_values) * 1)\n",
    "    subset.extend(label_values[:num_train])\n",
    "x_train_subset = x_train_array[subset]\n",
    "y_train_subset = y_train[subset]\n",
    "\n",
    "#To improve data convergence we will use a scaler to preprocess it\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_subset)\n",
    "x_test_scaled = scaler.transform(x_test_array)\n",
    "\n",
    "\n",
    "print(\"Number of samples in subsampled training set:\", x_train_subset.shape)\n",
    "\n",
    "#Start of loop for C values for Linear and RBF kernel\n",
    "for c in C_values:\n",
    "    linear_svm = SVC(kernel='linear', C=c, max_iter=500) #Classifier for linear SVM\n",
    "    linear_svm.fit(x_train_scaled, y_train_subset) #Train linear SVM\n",
    "    \n",
    "    #See the accuracy of linear SVM\n",
    "    accuracy = linear_svm.score(x_test_scaled, y_test)\n",
    "    print(f\"Accuracy with Linear SVM with C= {c}: {accuracy}\\n\")\n",
    "    \n",
    "    best_p = 0\n",
    "    best_accuracy = 0\n",
    "    #Start of loop for Parameter values for the RBF kernel\n",
    "    for p in param_values:\n",
    "        rbf_kernel_svm = SVC(kernel='rbf', gamma=p, C=c, max_iter=500)\n",
    "        rbf_kernel_svm.fit(x_train_scaled, y_train_subset)\n",
    "        rbf_accuracy = rbf_kernel_svm.score(x_test_scaled, y_test)\n",
    "        if rbf_accuracy>best_accuracy:\n",
    "            best_accuracy = rbf_accuracy\n",
    "            best_p = p\n",
    "        print(f\"Accuracy with RBF SVM with C= {c} and P={p}: {rbf_accuracy}\\n\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    print(f\"The best accuracy belongs to P={best_p}: {best_accuracy}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    "
   ],
   "id": "5d1652c162983397",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#CROSS ENTROPY VERSION WHICH WORKS!\n",
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100) #First layer with 100\n",
    "        self.fc2 = nn.Linear(100, 100) #Second layer\n",
    "        self.fc3 = nn.Linear(100, 50) #Third layer\n",
    "        self.fc4 = nn.Linear(50, num_classes) #Output layer\n",
    "        self.relu = nn.LeakyReLU() #LeakyRelU function\n",
    "        self.softmax = nn.Softmax(dim=1) #Softmax function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return self.softmax(out)\n",
    "    \n",
    "#Transform to Tensors\n",
    "x_train_tensor = t.tensor(x_train_array, dtype=t.float32)\n",
    "x_test_tensor = t.tensor(x_test_array, dtype=t.float32)\n",
    "y_train_tensor = t.tensor(y_train, dtype=t.long)\n",
    "y_test_tensor = t.tensor(y_test, dtype=t.long)\n",
    "\n",
    "#Define the input_size and num_classes and then create the FeedForwardNeuralNetwork\n",
    "input_size = x_train_tensor.shape[1]\n",
    "num_classes = len(t.unique(y_train_tensor))\n",
    "model = FeedForwardNeuralNetwork(input_size, num_classes)\n",
    "\n",
    "#Parameters print\n",
    "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_param}\\n\")\n",
    "\n",
    "#Loss and optimizer function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Batch and epochs initilizer with the loss and accuracy arrays\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "#Define the scallers for features and target variable\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "#Perform grid search\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "\n",
    "#Start of training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_correct = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "    epoch_test_correct = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(0, len(x_train_tensor), batch_size):\n",
    "        batch_x = t.tensor(x_train_tensor[i:i+batch_size], dtype=t.float32)\n",
    "        batch_y = t.tensor(y_train_tensor[i:i+batch_size], dtype=t.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x) #Trainer definition\n",
    "        loss = criterion(outputs, batch_y) #Loss definition\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() #Loss counter\n",
    "        \n",
    "        _, predicted = t.max(outputs, 1) #Accuracy counter\n",
    "        epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        \n",
    "        \n",
    "    #Model evaluation\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        for j in range(0, len(x_test_tensor), batch_size):\n",
    "            batch_test_x = t.tensor(x_test_tensor[j:j+batch_size], dtype=t.float32)\n",
    "            batch_test_y = t.tensor(y_test_tensor[j:j+batch_size], dtype=t.long)\n",
    "            y_pred = model(batch_test_x) #Trainer for test\n",
    "            loss_test = criterion(y_pred, batch_test_y) #Loss for test\n",
    "            epoch_test_loss += loss_test.item() #Loss counter\n",
    "            _, predicted_test = t.max(y_pred, 1) #Accuracy counter\n",
    "            epoch_test_correct += (predicted_test == batch_test_y).sum().item()\n",
    "        \n",
    "        \n",
    "    #Data classification to arrays for plot visualization\n",
    "    epoch_train_loss /= len(x_train_tensor)\n",
    "    epoch_train_accuracy = epoch_train_correct / len(x_train_tensor)\n",
    "    epoch_test_loss /= len(x_test_tensor)\n",
    "    epoch_test_accuracy = epoch_test_correct / len(x_test_tensor)\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "    test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "   \n",
    "\n"
   ],
   "id": "950645c788e69690",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#MSELOSS VERSION WHICH DOESNT WORK\n",
    "class FeedForwardNeuralNetworker(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(FeedForwardNeuralNetworker, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100) #First layer with 100\n",
    "        self.fc2 = nn.Linear(100, 100) #Second layer\n",
    "        self.fc3 = nn.Linear(100, 50) #Third layer\n",
    "        self.fc4 = nn.Linear(50, num_classes) #Output layer\n",
    "        self.relu = nn.LeakyReLU() #LeakyRelU function\n",
    "        self.softmax = nn.Softmax(dim=1) #Softmax function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return self.softmax(out)\n",
    "    \n",
    "\n",
    "\n",
    "#Define the input_size and num_classes and then create the FeedForwardNeuralNetworker\n",
    "input_size = x_train_tensor.shape[1]\n",
    "num_classes = len(t.unique(y_train_tensor))\n",
    "model = FeedForwardNeuralNetworker(input_size, num_classes)\n",
    "\n",
    "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_param}\\n\")\n",
    "\n",
    "#Loss and optimizer function\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Batch and epochs initilizer with the loss and accuracy arrays\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "#Define the scallers for features and target variable\n",
    "scaler_x = MinMaxScaler()\n",
    "x_train_scaled = scaler_x.fit_transform(x_train_array)\n",
    "x_test_scaled = scaler_x.transform(x_test_array)\n",
    "\n",
    "# Scale output values if needed\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Transform to Tensors\n",
    "x_train_tensor = t.tensor(x_train_scaled, dtype=t.float32)\n",
    "x_test_tensor = t.tensor(x_test_scaled, dtype=t.float32)\n",
    "y_train_tensor = t.tensor(y_train_scaled, dtype=t.float32)\n",
    "y_test_tensor = t.tensor(y_test_scaled, dtype=t.float32)\n",
    "\n",
    "#Perform grid search\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "\n",
    "#Start of training loop\n",
    "#num_batches = len(x_train_tensor) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_correct = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "    epoch_test_correct = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(0, len(x_train_tensor), batch_size):\n",
    "        batch_x = t.tensor(x_train_tensor[i:i+batch_size], dtype=t.float32)\n",
    "        batch_y = t.tensor(y_train_tensor[i:i+batch_size], dtype=t.float32).view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        _, predicted = t.max(outputs, 1)\n",
    "        epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "    #Model evaluation\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        for j in range(0, len(x_test_tensor), batch_size):\n",
    "            batch_test_x = t.tensor(x_test_tensor[j:j+batch_size], dtype=t.float32)\n",
    "            batch_test_y = t.tensor(y_test_tensor[j:j+batch_size], dtype=t.float32).view(-1, 1)\n",
    "            y_pred = model(batch_test_x)\n",
    "            loss_test = criterion(y_pred, batch_test_y)\n",
    "            epoch_test_loss += loss_test.item()\n",
    "            _, predicted_test = t.max(y_pred, 1)\n",
    "            epoch_test_correct += (predicted_test == batch_test_y).sum().item()\n",
    "        \n",
    "    \n",
    "    epoch_train_loss /= len(x_train_tensor)\n",
    "    epoch_train_accuracy = epoch_train_correct / len(x_train_tensor)\n",
    "    epoch_test_loss /= len(x_test_tensor)\n",
    "    epoch_test_accuracy = epoch_test_correct / len(x_test_tensor)\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "    test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "   \n",
    "        \n",
    "        \n",
    "        "
   ],
   "id": "941c1e6ac1685e99",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class ConvolutionalNeuralNetworkTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetworkTest, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1) #Input to first layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1) #First to second layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) #Second to third\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) #Max Pooling layer\n",
    "        self.flatten = nn.Flatten() #Important for fc1\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 100)  # Adjusted input size to match the output of conv2\n",
    "        self.dropout = nn.Dropout(0.3) #Dropout layer\n",
    "        self.fc2 = nn.Linear(100, 10)  # Output size matches the number of classes\n",
    "        self.softmax = nn.Softmax() #Softmax layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = t.relu(self.conv1(x))\n",
    "        out = t.relu(self.conv2(out))\n",
    "        out = self.pool(out)\n",
    "        out = t.relu(self.conv3(out))\n",
    "        out = self.flatten(out)\n",
    "        out = t.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.softmax(out)\n",
    "        \n",
    "#Regulate the shape of the images before the transformation to tensor\n",
    "x_train_imager = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test_imager = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "#Turn to tensors the train and test data with the correct order of data\n",
    "x_train_torch = t.tensor(x_train_imager, dtype=t.float32).permute(0, 3, 1, 2)\n",
    "x_test_torch = t.tensor(x_test_imager, dtype=t.float32).permute(0, 3, 1, 2)\n",
    "y_train_torch = t.tensor(y_train, dtype=t.long)\n",
    "y_test_torch = t.tensor(y_test, dtype=t.long)\n",
    "\n",
    "#Model, criterion and optimizer definition\n",
    "modeler = ConvolutionalNeuralNetworkTest()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modeler.parameters(), lr=0.001)\n",
    "\n",
    "#Important definitions\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "#Start of training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_correct = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "    epoch_test_correct = 0.0\n",
    "    modeler.train()\n",
    "    for i in range(0, len(x_train_torch), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = x_train_torch[i:i+batch_size]\n",
    "        batch_y = y_train_torch[i:i+batch_size]\n",
    "        \n",
    "        outputs = modeler(batch_x) #Train definition\n",
    "        loss = criterion(outputs, batch_y) #Loss definition\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() #Loss counter\n",
    "        _, predicted = t.max(outputs, 1) #Accuracy counter\n",
    "        epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "    #Start of test loop\n",
    "    modeler.eval()\n",
    "    with t.no_grad():\n",
    "        for j in range(0, len(x_test_torch), batch_size):\n",
    "            batch_x = x_test_torch[j:j+batch_size]\n",
    "            batch_y = y_test_torch[j:j+batch_size]\n",
    "            outputs = modeler(batch_x) #Trainer for test\n",
    "            loss = criterion(outputs, batch_y) #Loss for test\n",
    "            epoch_test_loss += loss.item() #Loss counter\n",
    "            _, predicted = t.max(outputs, 1) #Accuracy counter\n",
    "            epoch_test_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    #Data classification to arrays for plot visualization\n",
    "    epoch_train_loss /= len(x_train_torch)\n",
    "    epoch_train_accuracy = epoch_train_correct / len(x_train_torch)\n",
    "    epoch_test_loss /= len(x_test_torch)\n",
    "    epoch_test_accuracy = epoch_test_correct / len(x_test_torch)\n",
    "    \n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "    test_accuracies.append(epoch_test_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "368558c2af8f5fdd",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
